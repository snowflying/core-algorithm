- Problem Description:
Starting on 2021-10-06T18:30Z customer noticed kubectl commands failing and their application is unable to establish communication with the API server to update it's components to establish communication between pods, services, etc.
 
PG engaged and pointed out high levels of API server throttling stemming from Java user agents:
The cluster API server is overloaded since yesterday at 19:20 aprox. and most of the calls are being throttled:
 
 
https://portal.microsoftgeneva.com/s/A8201F41?overrides=[{"query":"//*[id='Region']","key":"value","replacement":""},{"query":"//*[id='UnderlayName']","key":"value","replacement":""},{"query":"//*[id='container_name']","key":"value","replacement":""},{"query":"//*[id='namespace']","key":"value","replacement":"60054d3f42c1a2000199cfeb"},{"query":"//*[id='container']","key":"value","replacement":""}]%20
 
 
 
Top throttled requests come from these two agents:
 
URI	User Agent / Username	Resource	Verb	Throttled
/api/v1/namespaces/prd-eu2/pods	Java/11.0.12
system:serviceaccount:prd-eu2:hybris-cluster-service-account	pods	list	4,599,471
/api/v1/namespaces/prd-eu5/pods	Java/11.0.12
system:serviceaccount:prd-eu5:hybris-cluster-service-account	pods	list	4,409,099
 
It seems that this is not the first time this happens, Case 2109170050000100 was opened on Sept 17th on the same issue. In the associated icm:  https://portal.microsofticm.com/imp/v3/incidents/details/265762892/home there is this final comment:
 
ASI's API Server Details page shows huge load on the API server at the time of the problem (around 03:00 UTC on the 17th of Sept).  It is coming from list calls, made by a Java app running under a number of accounts with names like system:serviceaccount:prd-euX:hybris-cluster-service-account (where X is a digit from 1 to 5). 
 
The calls are listing pods.  And to make matters worse, the calls are unchunked (i.e. they are not using the limit and continue params supported by recent versions of K8s).  At peak times these calls re being made at rates over 500 per second.  That's really, really high for list queries (which are the most resource intensive type of queries).
 
In about 4 months of looking at API server load issues, I don't recall ever seeing such consistent high listing load from any other application.  So I'd suggest that this application needs to be modified to reduce the number of listing calls that it makes.  Maybe using a K8s informer pattern would help.
 
Since the issue started the API server was restarted by the previous CO but it didn't help to mitigate. Also they scaled out the nodes since there seems to be a node overloading issue too, but it did not help either.
 
The customer agreed that the issue seems to be the application and they are asking for a downtime to restart the application and see if that solves the issue. We are monitoring for the outcome.
 
 
Customer is planning to restart app at 2AM CET on October 9th 2021 (2021-10-09T00:00Z) and need AKS team on bridge when doing so to further debug.
 
AKS:
/subscriptions/581e4fcb-c080-4c0b-8269-763fe2283808/resourceGroups/GOE-PRD-AKSEU-rg/providers/Microsoft.ContainerService/managedClusters/goe-prd-akseu
 
- Customer Contact Details: 
Name: Sourav Jha
Email: HM-IO-IaaS@accenture.com
Phone: +91 9008371600
 
- Completed Action(s) and Result(s):
Joined previous bridge check about kube-apiserver's status in link:
https://portal.microsoftgeneva.com/s/A6972605?overrides=[{"query":"//*[id='Region']","key":"value","replacement":""},{"query":"//*[id='UnderlayName']","key":"value","replacement":""},{"query":"//*[id='container_name']","key":"value","replacement":""},{"query":"//*[id='namespace']","key":"value","replacement":"60054d3f42c1a2000199cfeb"},{"query":"//*[id='container']","key":"value","replacement":""}]%20

Joined call with customer to discuss case.
let customer startup their deployment after kube-apiserver restart.
till the time 03:30 UTC 09/10, all deployment get start with on issue and exception shown, also check the kube-apiserver's running status is well also.
so lowerdown the security and keep this ticket for two days to find whether this issue happend again
 
- IcM #:
https://portal.microsofticm.com/imp/v3/incidents/details/265762892/home
 
- Current Status: 
the those pods in aks cluster turn back well after restart. waiting for two days to see further
 
- Next Actions:
 
- Action on Next Engineer/Customer -  
check with cx Tuesday to know if this issue happend again, or standby if customer meet the problem again and they will raise serverity to A
